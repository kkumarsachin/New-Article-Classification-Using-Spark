{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # News Article Classification\n",
    "  \n",
    "  *This notebook demonstrates the implementation of News article classification in Spark using mllib*\n",
    "\n",
    "1. The articles are downloaded from the New York times using NYT API from python. The code for that is avaliable in another notebook. \n",
    "2. The Downloaded data is in **news_data** folder with individual categories as the names of the subfolders\n",
    "3. The articles are in text format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Importing Libraries\n",
    "\n",
    "- In this step we import all the required libraries from pyspark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF,StopWordsRemover,IDF,Tokenizer\n",
    "from pyspark.ml.classification import LogisticRegression,RandomForestClassifier,NaiveBayes\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Spark Context\n",
    "\n",
    "- Here we instantiate the spark context. Please note that i have commented below because When i launch pyspark from cmd sc is already created.If it doesnt then we can create as per below commented line\n",
    "- Then i use SQL context which will be used later to create Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#When i launch pyspark from cmd sc is already created. So i dont have to use sparkcontext. \n",
    "#sc = SparkContext(\"local[4]\",\"news_analysis\")\n",
    "sqlContext = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Reading data and Converting to Dataframes\n",
    "\n",
    "- Now i read the files as wholetext files which creates a PairRDD as (filepath,text)\n",
    "- Then i will create the schema from the files read.\n",
    "- I will create a schema with header news_id,news_category,news_text\n",
    "- path[0] has the filepath . I split the function and use rightmost as id(which is the name of the text file) , 2nd rightmost as category which is the name of the subfolders\n",
    "- path[1] gives me the text data of the file\n",
    "- Using the Dataframe API i create the dataframe\n",
    "- The sample Data is shown as the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+--------------------+\n",
      "| news_id|news_category|           news_text|\n",
      "+--------+-------------+--------------------+\n",
      "|1000.txt|     Business|From the outset, ...|\n",
      "|1108.txt|     Business|Most presidents w...|\n",
      "|1195.txt|     Business|The Trump adminis...|\n",
      "|1207.txt|     Business|LONDON � If an au...|\n",
      "|  50.txt|     Business|WASHINGTON � Pres...|\n",
      "| 513.txt|     Business|Each week, Farhad...|\n",
      "| 522.txt|     Business|Delta Air Lines s...|\n",
      "| 673.txt|     Business|FRANKFURT � Polit...|\n",
      "| 698.txt|     Business|TOKYO � Getting h...|\n",
      "| 700.txt|       Movies|Handsome cinemato...|\n",
      "| 767.txt|       Movies|This year�s Acade...|\n",
      "| 863.txt|       Movies|LOS ANGELES � Opr...|\n",
      "| 907.txt|       Movies|On Wednesday, tho...|\n",
      "|1037.txt|     Politics|WASHINGTON � Pres...|\n",
      "|1071.txt|     Politics|WASHINGTON � When...|\n",
      "| 314.txt|     Politics|This is the first...|\n",
      "| 323.txt|     Politics|ROME � It happene...|\n",
      "| 325.txt|     Politics|Gov. Andrew M. Cu...|\n",
      "| 411.txt|     Politics|WASHINGTON � A ye...|\n",
      "| 425.txt|     Politics|WASHINGTON � The ...|\n",
      "| 977.txt|     Politics|WASHINGTON � Gary...|\n",
      "| 100.txt|       Sports|FRONT PAGEA map w...|\n",
      "| 109.txt|       Sports|Everyone knows Bo...|\n",
      "| 119.txt|       Sports|The N.F.L. said T...|\n",
      "| 154.txt|       Sports|Virginia (31-2) c...|\n",
      "| 182.txt|       Sports|GLENDALE, Ariz. �...|\n",
      "| 250.txt|       Sports|BALTIMORE � The s...|\n",
      "| 254.txt|       Sports|�I love volleybal...|\n",
      "|   4.txt|       Sports|SPORTSAn article ...|\n",
      "|  76.txt|       Sports|We recently reach...|\n",
      "+--------+-------------+--------------------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Reading the path and data and splitting for creating dataframe\n",
    "#https://spark.apache.org/docs/2.2.0/api/java/org/apache/spark/SparkContext.html#wholeTextFiles-java.lang.String-int-\n",
    "news_data = sc.wholeTextFiles(\"News_Data/*\")\n",
    "news_schema = StructType([\n",
    "                    StructField(\"news_id\" , StringType(), True),\n",
    "                    StructField(\"news_category\" , StringType(), True),\n",
    "                    StructField(\"news_text\" , StringType(), True)\n",
    "                    ])\n",
    " \n",
    "news_split = news_data.map(lambda path :  (path[0].split(\"/\")[-1], path[0].split(\"/\")[-2],path[1]))\n",
    "news_dataframe = sqlContext.createDataFrame(news_split,news_schema)\n",
    "news_dataframe.sample(False,0.1).show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Labels\n",
    "Here i use stringindexer to encode categorical labels to numeric labels. Few sample data is shown as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+--------------------+-----+\n",
      "| news_id|news_category|           news_text|label|\n",
      "+--------+-------------+--------------------+-----+\n",
      "|1004.txt|     Business|One appointee wen...|  0.0|\n",
      "|1042.txt|     Business|FRANKFURT � Polit...|  0.0|\n",
      "|1122.txt|     Business|A new chapter ope...|  0.0|\n",
      "|1162.txt|     Business|The Abraaj Group ...|  0.0|\n",
      "|1202.txt|     Business|On a Thursday eve...|  0.0|\n",
      "| 478.txt|     Business|JERUSALEM � Seven...|  0.0|\n",
      "| 522.txt|     Business|Delta Air Lines s...|  0.0|\n",
      "| 575.txt|     Business|A possible merger...|  0.0|\n",
      "| 701.txt|     Business|For weeks, users ...|  0.0|\n",
      "| 531.txt|       Movies|With accusations ...|  2.0|\n",
      "| 704.txt|       Movies|LOS ANGELES � �Th...|  2.0|\n",
      "| 707.txt|       Movies|The Oscars turn 9...|  2.0|\n",
      "| 757.txt|       Movies|Since sexual hara...|  2.0|\n",
      "| 930.txt|       Movies|Say hello to this...|  2.0|\n",
      "| 962.txt|       Movies|The �we� in �What...|  2.0|\n",
      "|1016.txt|     Politics|WASHINGTON � Gary...|  1.0|\n",
      "|1028.txt|     Politics|President Trump�s...|  1.0|\n",
      "|1030.txt|     Politics|WASHINGTON � The ...|  1.0|\n",
      "| 173.txt|     Politics|BEIJING � If Pres...|  1.0|\n",
      "| 271.txt|     Politics|WASHINGTON � Earl...|  1.0|\n",
      "| 318.txt|     Politics|This is the secon...|  1.0|\n",
      "| 387.txt|     Politics|For beleaguered c...|  1.0|\n",
      "| 411.txt|     Politics|WASHINGTON � A ye...|  1.0|\n",
      "| 446.txt|     Politics|BEIJING � One Sun...|  1.0|\n",
      "| 453.txt|     Politics|For Gov. Andrew M...|  1.0|\n",
      "| 494.txt|     Politics|This is the third...|  1.0|\n",
      "| 989.txt|     Politics|TOKYO � Takako Su...|  1.0|\n",
      "| 993.txt|     Politics|DAYTON, Ohio � At...|  1.0|\n",
      "| 120.txt|       Sports|INDIAN WELLS, Cal...|  3.0|\n",
      "| 126.txt|       Sports|Madison Square Ga...|  3.0|\n",
      "+--------+-------------+--------------------+-----+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "label_stringIdx = StringIndexer(inputCol = \"news_category\", outputCol = \"label\")\n",
    "indexed = label_stringIdx.fit(news_dataframe).transform(news_dataframe)\n",
    "indexed.sample(False,0.1).show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and Feature Extraction\n",
    "- Here we use tokenizer to convert sentence to words or tokens\n",
    "- Stop words to remove irrelevant words\n",
    "- I use hashingTF to hash the features so that it will be faster and easier\n",
    "- Then we use TF-IDF for feature extraction.For more information regarding TF-IDF use link \n",
    "https://spark.apache.org/docs/2.2.0/mllib-feature-extraction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the below code is taken from pipeline.example from pyspark example program and\n",
    "#https://spark.apache.org/docs/1.6.1/ml-features.html#tf-idf-hashingtf-and-idf\n",
    "\n",
    "train_set, test_set = indexed.randomSplit([0.8,0.2])\n",
    "tokenizer = Tokenizer(inputCol=\"news_text\", outputCol=\"news_words\")\n",
    "remover = StopWordsRemover(inputCol=\"news_words\", outputCol=\"filtered_words\",caseSensitive=False)\n",
    "hashingTF = HashingTF(inputCol=\"filtered_words\",outputCol=\"hashed_features\",numFeatures=1000)\n",
    "idf = IDF(inputCol=\"hashed_features\",outputCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multi-Class Classification using Decision Tree Classifier\n",
    "- Here i instantiate Decision Tree Classifer and create a pipeline using it\n",
    "- Then i use my training set to train the model\n",
    "- I use the testing set to test my model and the sample output is given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----------+-----+\n",
      "| news_id|news_category|prediction|label|\n",
      "+--------+-------------+----------+-----+\n",
      "|1006.txt|     Politics|       1.0|  1.0|\n",
      "|1014.txt|     Business|       0.0|  0.0|\n",
      "|1016.txt|     Politics|       1.0|  1.0|\n",
      "|1026.txt|     Politics|       2.0|  1.0|\n",
      "|1034.txt|     Politics|       0.0|  1.0|\n",
      "|1037.txt|     Politics|       1.0|  1.0|\n",
      "|1051.txt|     Politics|       3.0|  1.0|\n",
      "|1071.txt|     Politics|       1.0|  1.0|\n",
      "|1115.txt|     Business|       0.0|  0.0|\n",
      "|1164.txt|     Business|       0.0|  0.0|\n",
      "+--------+-------------+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree Classifier\n",
    "#https://spark.apache.org/docs/1.5.0/ml-decision-tree.html\n",
    "DC = DecisionTreeClassifier(labelCol = \"label\",featuresCol = \"features\")\n",
    "DC_pipeline = Pipeline(stages=[tokenizer, remover,hashingTF, idf,DC])\n",
    "DC_model = DC_pipeline.fit(train_set)\n",
    "DC_predictions = DC_model.transform(test_set)\n",
    "DC_predictions.select(\"news_id\",\"news_category\",\"prediction\",\"label\").show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Accuracy using Decision Tree Classifer\n",
    "We use evaluators to predict the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6723382376206296"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(DC_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-Class Classification using Random Forest Classifier\n",
    "- Here i instantiate Random Forest Classifier and create a pipeline using it\n",
    "- Then i use my training set to train the model\n",
    "- I use the testing set to test my model and the sample output is given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----------+-----+\n",
      "| news_id|news_category|prediction|label|\n",
      "+--------+-------------+----------+-----+\n",
      "|1006.txt|     Politics|       1.0|  1.0|\n",
      "|1014.txt|     Business|       0.0|  0.0|\n",
      "|1016.txt|     Politics|       1.0|  1.0|\n",
      "|1026.txt|     Politics|       1.0|  1.0|\n",
      "|1034.txt|     Politics|       1.0|  1.0|\n",
      "|1037.txt|     Politics|       1.0|  1.0|\n",
      "|1051.txt|     Politics|       1.0|  1.0|\n",
      "|1071.txt|     Politics|       1.0|  1.0|\n",
      "|1115.txt|     Business|       1.0|  0.0|\n",
      "|1164.txt|     Business|       0.0|  0.0|\n",
      "|1206.txt|     Business|       0.0|  0.0|\n",
      "|   2.txt|     Business|       0.0|  0.0|\n",
      "| 281.txt|       Movies|       2.0|  2.0|\n",
      "| 288.txt|     Politics|       2.0|  1.0|\n",
      "| 313.txt|     Politics|       1.0|  1.0|\n",
      "| 473.txt|     Business|       0.0|  0.0|\n",
      "| 481.txt|     Business|       0.0|  0.0|\n",
      "|  50.txt|     Business|       0.0|  0.0|\n",
      "| 511.txt|     Business|       0.0|  0.0|\n",
      "| 610.txt|     Business|       0.0|  0.0|\n",
      "+--------+-------------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#https://spark.apache.org/docs/2.1.0/ml-classification-regression.html\n",
    "RF = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n",
    "RF_pipeline = Pipeline(stages=[tokenizer, remover,hashingTF, idf,RF])\n",
    "RF_model = RF_pipeline.fit(train_set)\n",
    "RF_predictions = RF_model.transform(test_set)\n",
    "RF_predictions.select(\"news_id\",\"news_category\",\"prediction\",\"label\").show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Accuracy using Random Forest Classifer\n",
    "We use evaluators to predict the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7335654479565111"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(RF_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-Class Classification using Naive Bayes\n",
    "- Here i instantiate Naive Bayes and create a pipeline using it\n",
    "- Then i use my training set to train the model\n",
    "- I use the testing set to test my model and the sample output is given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----------+-----+\n",
      "| news_id|news_category|prediction|label|\n",
      "+--------+-------------+----------+-----+\n",
      "|1006.txt|     Politics|       1.0|  1.0|\n",
      "|1014.txt|     Business|       1.0|  0.0|\n",
      "|1016.txt|     Politics|       1.0|  1.0|\n",
      "|1026.txt|     Politics|       1.0|  1.0|\n",
      "|1034.txt|     Politics|       0.0|  1.0|\n",
      "|1037.txt|     Politics|       1.0|  1.0|\n",
      "|1051.txt|     Politics|       1.0|  1.0|\n",
      "|1071.txt|     Politics|       1.0|  1.0|\n",
      "|1115.txt|     Business|       0.0|  0.0|\n",
      "|1164.txt|     Business|       0.0|  0.0|\n",
      "|1206.txt|     Business|       0.0|  0.0|\n",
      "|   2.txt|     Business|       0.0|  0.0|\n",
      "| 281.txt|       Movies|       2.0|  2.0|\n",
      "| 288.txt|     Politics|       1.0|  1.0|\n",
      "| 313.txt|     Politics|       1.0|  1.0|\n",
      "| 473.txt|     Business|       0.0|  0.0|\n",
      "| 481.txt|     Business|       0.0|  0.0|\n",
      "|  50.txt|     Business|       1.0|  0.0|\n",
      "| 511.txt|     Business|       0.0|  0.0|\n",
      "| 610.txt|     Business|       0.0|  0.0|\n",
      "+--------+-------------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#https://spark.apache.org/docs/2.1.0/ml-classification-regression.html\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "nb_pipeline = Pipeline(stages=[tokenizer, remover,hashingTF, idf,nb])\n",
    "nb_model = nb_pipeline.fit(train_set)\n",
    "nb_predictions = nb_model.transform(test_set)\n",
    "nb_predictions.select(\"news_id\",\"news_category\",\"prediction\",\"label\").show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Accuracy using Naive Bayes\n",
    "We use evaluators to predict the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7476621417797887"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(nb_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Class Classification using Logistic Regression\n",
    "- Here i instantiate Logistic Regression and create a pipeline using it\n",
    "- Then i use my training set to train the model\n",
    "- I use the testing set to test my model and the sample output is given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+--------------------+----------+-----+\n",
      "| news_id|news_category|         probability|prediction|label|\n",
      "+--------+-------------+--------------------+----------+-----+\n",
      "|1006.txt|     Politics|[2.87055638133401...|       1.0|  1.0|\n",
      "|1014.txt|     Business|[0.94630158747331...|       0.0|  0.0|\n",
      "|1016.txt|     Politics|[2.87055638133401...|       1.0|  1.0|\n",
      "|1026.txt|     Politics|[5.65882720967919...|       1.0|  1.0|\n",
      "|1034.txt|     Politics|[0.47128466297770...|       0.0|  1.0|\n",
      "|1037.txt|     Politics|[7.19995614450264...|       1.0|  1.0|\n",
      "|1051.txt|     Politics|[1.82821913164354...|       1.0|  1.0|\n",
      "|1071.txt|     Politics|[7.70170280532057...|       1.0|  1.0|\n",
      "|1115.txt|     Business|[0.99537068551977...|       0.0|  0.0|\n",
      "|1164.txt|     Business|[0.96529871014097...|       0.0|  0.0|\n",
      "|1206.txt|     Business|[0.98851652259368...|       0.0|  0.0|\n",
      "|   2.txt|     Business|[0.99994490550686...|       0.0|  0.0|\n",
      "| 281.txt|       Movies|[7.76175080894985...|       2.0|  2.0|\n",
      "| 288.txt|     Politics|[0.06102331679013...|       2.0|  1.0|\n",
      "| 313.txt|     Politics|[0.02335163959745...|       1.0|  1.0|\n",
      "| 473.txt|     Business|[0.99392055849689...|       0.0|  0.0|\n",
      "| 481.txt|     Business|[0.99317292270867...|       0.0|  0.0|\n",
      "|  50.txt|     Business|[0.97773001800666...|       0.0|  0.0|\n",
      "| 511.txt|     Business|[0.58682060067010...|       0.0|  0.0|\n",
      "| 610.txt|     Business|[0.97931106212731...|       0.0|  0.0|\n",
      "+--------+-------------+--------------------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression. This code is taken from Logistic regression summary example from the spark\n",
    "LR = LogisticRegression(maxIter=10)\n",
    "LR_pipeline = Pipeline(stages=[tokenizer, remover,hashingTF, idf,LR])\n",
    "LR_model = LR_pipeline.fit(train_set)\n",
    "LR_predictions = LR_model.transform(test_set)\n",
    "LR_predictions.select(\"news_id\",\"news_category\",\"probability\",\"prediction\",\"label\").show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Accuracy using Logistic Regression\n",
    "We use evaluators to predict the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8239795918367346"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(LR_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best model is Logistic regression as the accuracy is higher.  I will be using the two models and now in the next step we will use this model and try to optimize the parameters for higher accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter tuning for  Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "DC_paramGrid = ParamGridBuilder().addGrid(hashingTF.numFeatures, [100, 1000, 10000]).addGrid(DC.maxDepth, [1, 2, 6, 10]).addGrid(DC.maxBins, [20, 40, 80]).build()\n",
    "crossval = CrossValidator(estimator=DC_pipeline,estimatorParamMaps=DC_paramGrid,evaluator=evaluator,numFolds=10)\n",
    "DC_cvModel = crossval.fit(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Accuracy for Tuned Decision Tree Classifer\n",
    "As the accuracy suggests the tuned model performs better than untuned model. So we will use tuned model to use to predict our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7509109872175966"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DC_cv_predictions = DC_cvModel.transform(test_set)\n",
    "evaluator.evaluate(DC_cv_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter tuning for  Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from crossvalidation example from spark\n",
    "paramGrid = ParamGridBuilder().addGrid(hashingTF.numFeatures, [100, 1000, 10000]).addGrid(LR.regParam, [0.1, 0.3, 0.5]).addGrid(LR.elasticNetParam, [0.1, 0.2, 0.4]).build()\n",
    "crossval = CrossValidator(estimator=LR_pipeline,estimatorParamMaps=paramGrid,evaluator=evaluator,numFolds=10)\n",
    "cvModel = crossval.fit(train_set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Accuracy for Tuned  Logistic Regression\n",
    "As the accuracy suggests the tuned model performs better than untuned model. So we will use tuned model to use to predict our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8793422682000509"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_predictions = cvModel.transform(test_set)\n",
    "evaluator.evaluate(cv_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The Tuned model of Logistic Regression is the best two models for news classification. I will be using Tuned Decision Classifer as a second classifier to compare with Logistic regression model*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Some Unknown data for Grading Purpose\n",
    "- This part can be used by TA for testing the model\n",
    "- Here i am using the two tuned model ie Tuned Decsion tree and Tuned logistic regression \n",
    "- The article that can be used for testing is placed in the test_data folder insid eunknown folder\n",
    "- Note that the article present in test data has not been used for training or testing purpose while buliding model\n",
    "- If TA wants to verify with their own data then please keep the data in test folder as i have done\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+--------------------+-----+\n",
      "|       news_id|news_category|           news_text|label|\n",
      "+--------------+-------------+--------------------+-----+\n",
      "| Business1.txt|      Unknown|LOS ANGELES � In ...|  0.0|\n",
      "| Business2.txt|      Unknown|The working world...|  0.0|\n",
      "| Business3.txt|      Unknown|WASHINGTON � Afte...|  0.0|\n",
      "| Business4.txt|      Unknown|With its $11.9 bi...|  0.0|\n",
      "| Business5.txt|      Unknown|LONDON � During h...|  0.0|\n",
      "|   Movies1.txt|      Unknown|The title of �Mai...|  0.0|\n",
      "|   Movies2.txt|      Unknown|�The Leisure Seek...|  0.0|\n",
      "|   Movies3.txt|      Unknown|Here�s a look at ...|  0.0|\n",
      "|   Movies4.txt|      Unknown|Finding an opport...|  0.0|\n",
      "|   Movies5.txt|      Unknown|Here�s a look at ...|  0.0|\n",
      "| Politics1.txt|      Unknown|For Gov. Andrew M...|  0.0|\n",
      "|Politics15.txt|      Unknown|PLAINVILLE, Conn....|  0.0|\n",
      "| Politics2.txt|      Unknown|SEOUL, South Kore...|  0.0|\n",
      "| Politics3.txt|      Unknown|WASHINGTON � Befo...|  0.0|\n",
      "| Politics4.txt|      Unknown|Elizabeth Burnat ...|  0.0|\n",
      "|   Sports1.txt|      Unknown|PYEONGCHANG, Sout...|  0.0|\n",
      "|   Sports2.txt|      Unknown|INDIAN WELLS, Cal...|  0.0|\n",
      "|   Sports3.txt|      Unknown|Zavier Simpson le...|  0.0|\n",
      "|   Sports4.txt|      Unknown|MANCHESTER, Engla...|  0.0|\n",
      "|   Sports5.txt|      Unknown|MOSCOW � Britain�...|  0.0|\n",
      "+--------------+-------------+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#For testing purpose\n",
    "from pyspark.sql.types import *\n",
    "test_data = sc.wholeTextFiles(\"test_data/*\")\n",
    "test_schema = StructType([\n",
    "                    StructField(\"news_id\" , StringType(), True),\n",
    "                    StructField(\"news_category\" , StringType(), True),\n",
    "                    StructField(\"news_text\" , StringType(), True)\n",
    "                    ])\n",
    " \n",
    "t = test_data.map(lambda path :  (path[0].split(\"/\")[-1], path[0].split(\"/\")[-2],path[1]))\n",
    "test_dataframe = sqlContext.createDataFrame(t,test_schema)\n",
    "label1_stringIdx = StringIndexer(inputCol = \"news_category\", outputCol = \"label\")\n",
    "indexed1 = label1_stringIdx.fit(test_dataframe).transform(test_dataframe)\n",
    "indexed1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifing Test Data using Cross validated Decision Tree Classifier\n",
    "- For verifing we can compare the name of the file and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+--------------------+----------+\n",
      "|       news_id|news_category|         probability|prediction|\n",
      "+--------------+-------------+--------------------+----------+\n",
      "| Business1.txt|      Unknown|[0.90196078431372...|       0.0|\n",
      "| Business2.txt|      Unknown|   [0.0,0.0,0.0,1.0]|       3.0|\n",
      "| Business3.txt|      Unknown|   [0.0,1.0,0.0,0.0]|       1.0|\n",
      "| Business4.txt|      Unknown|   [0.0,0.0,1.0,0.0]|       2.0|\n",
      "| Business5.txt|      Unknown|[0.90196078431372...|       0.0|\n",
      "|   Movies1.txt|      Unknown|   [0.0,0.0,1.0,0.0]|       2.0|\n",
      "|   Movies2.txt|      Unknown|   [0.0,0.0,1.0,0.0]|       2.0|\n",
      "|   Movies3.txt|      Unknown|   [0.0,0.0,1.0,0.0]|       2.0|\n",
      "|   Movies4.txt|      Unknown|   [0.0,0.0,1.0,0.0]|       2.0|\n",
      "|   Movies5.txt|      Unknown|   [0.0,0.0,1.0,0.0]|       2.0|\n",
      "| Politics1.txt|      Unknown|   [0.0,1.0,0.0,0.0]|       1.0|\n",
      "|Politics15.txt|      Unknown|[0.90196078431372...|       0.0|\n",
      "| Politics2.txt|      Unknown|   [0.0,1.0,0.0,0.0]|       1.0|\n",
      "| Politics3.txt|      Unknown|   [0.0,1.0,0.0,0.0]|       1.0|\n",
      "| Politics4.txt|      Unknown|   [0.0,0.0,1.0,0.0]|       2.0|\n",
      "|   Sports1.txt|      Unknown|   [1.0,0.0,0.0,0.0]|       0.0|\n",
      "|   Sports2.txt|      Unknown|   [1.0,0.0,0.0,0.0]|       0.0|\n",
      "|   Sports3.txt|      Unknown|   [0.0,0.0,0.0,1.0]|       3.0|\n",
      "|   Sports4.txt|      Unknown|   [0.0,0.0,1.0,0.0]|       2.0|\n",
      "|   Sports5.txt|      Unknown|[0.90196078431372...|       0.0|\n",
      "+--------------+-------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predictions = DC_cvModel.transform(indexed1)\n",
    "test_predictions.select(\"news_id\",\"news_category\",\"probability\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifing Test Data using Cross validated Logistic Regression model\n",
    "- For verifing we can compare the name of the file and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+--------------------+----------+\n",
      "|       news_id|news_category|         probability|prediction|\n",
      "+--------------+-------------+--------------------+----------+\n",
      "| Business1.txt|      Unknown|[0.63437357374768...|       0.0|\n",
      "| Business2.txt|      Unknown|[0.32044585695464...|       2.0|\n",
      "| Business3.txt|      Unknown|[0.24021041446877...|       1.0|\n",
      "| Business4.txt|      Unknown|[0.30522460503989...|       0.0|\n",
      "| Business5.txt|      Unknown|[0.30520737583808...|       0.0|\n",
      "|   Movies1.txt|      Unknown|[0.28281169688546...|       2.0|\n",
      "|   Movies2.txt|      Unknown|[0.23598544692892...|       2.0|\n",
      "|   Movies3.txt|      Unknown|[0.04742457198801...|       2.0|\n",
      "|   Movies4.txt|      Unknown|[0.23669207009264...|       2.0|\n",
      "|   Movies5.txt|      Unknown|[0.04742457198801...|       2.0|\n",
      "| Politics1.txt|      Unknown|[0.22250325027191...|       1.0|\n",
      "|Politics15.txt|      Unknown|[0.15856301869865...|       1.0|\n",
      "| Politics2.txt|      Unknown|[0.06159611122567...|       1.0|\n",
      "| Politics3.txt|      Unknown|[0.26125966681277...|       1.0|\n",
      "| Politics4.txt|      Unknown|[0.24261255877288...|       1.0|\n",
      "|   Sports1.txt|      Unknown|[0.18094044112114...|       3.0|\n",
      "|   Sports2.txt|      Unknown|[0.07104212952420...|       3.0|\n",
      "|   Sports3.txt|      Unknown|[0.12009161104770...|       3.0|\n",
      "|   Sports4.txt|      Unknown|[0.18381814049787...|       3.0|\n",
      "|   Sports5.txt|      Unknown|[0.19304395069566...|       3.0|\n",
      "+--------------+-------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predictions = cvModel.transform(indexed1)\n",
    "test_predictions.select(\"news_id\",\"news_category\",\"probability\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "As you can see we get a good classification by using Logistic Regression.But still we can find that there are cases which is not predicted right. This can be improved.\n",
    "- Since the input datasets contains only 100 data for each category we can definitely improve with increase in datasets\n",
    "- Also there are articles which appear in both the category ie something like \"Trump increasing Tarrifs\" which comes under both Politics and Business . So this can be accepted in both\n",
    "- Also since the data is taken only for a particular period we can expect only particular kind of data present. This can be overcome with improved datasets\n",
    "\n",
    "For the given problem of News Classifiation and Dataset we can see that Tuned Logistic regression is the  best model. We have also verified the model with unknown dataset which was not used to training or testing the model.\n",
    "Thus i have creted a pipelined model . I have completed the five main stages as per requirement of this assignment ie:      Collecting and Cleaning, feature extracting , Multiclass classification, Testing and Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTHOR : Sachin Kumar Kuppayya"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
